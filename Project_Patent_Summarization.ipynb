{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Patent_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leshow123/An-Abstractive-Text-Summarizer-For-Patent-Documents/blob/main/Project_Patent_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je2VjFFxF3Ox",
        "outputId": "92d74dd0-3ce9-4612-f851-79ad8eaad2c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -q -U tensor2tensor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 30.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 3.5MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 64.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 686kB 64.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 67.8MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJpOsOn33TSo"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea1ef2DRamRt"
      },
      "source": [
        "def dataset_update(dataset_in_cloud='/content/drive/My Drive/sample.csv'):\n",
        "  \"\"\" Use this to extract 'unclean' data from IamIPAB's raw dump \"\"\"\n",
        "  import glob\n",
        "  import pandas as pd\n",
        "  \n",
        "  #TODO : Handle file not found scenario etc.\n",
        "  \n",
        "  file_path = glob.glob(dataset_in_cloud, recursive=True).pop(0)\n",
        "  print(file_path)\n",
        "  df = pd.read_csv(file_path,sep=';',encoding='latin-1', error_bad_lines=False,\n",
        "                   warn_bad_lines=False, header=0)\n",
        "  number_of_columns = len(list(df.columns))\n",
        "  datapoints = int(df.size/number_of_columns)\n",
        "  \n",
        "  df[:datapoints].to_csv('/content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_{}_dp.csv'.format(datapoints), sep=';')\n",
        "  print('INFO:Finished writing sample_{}_dp.csv'.format(datapoints) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6A6snWn3asf"
      },
      "source": [
        "def touch(path):\n",
        "  \n",
        "  success = True\n",
        "  try:\n",
        "    with open(path, 'a') as en_dataset:\n",
        "      en_dataset.write('Title'+'@'+'Abstract'+'@'+'Claims'+'@'+'Description'+'\\n')\n",
        "  except:\n",
        "    success = False\n",
        "  \n",
        "  return success"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynig9CErL3vT"
      },
      "source": [
        "def extract_english(*args):\n",
        "  \n",
        "  # Reference 'most common by freq': https://www.wordfrequency.info/free.asp?s=y\n",
        "  # Note: Took out 'in' as its prevalent in German too;\n",
        "  #       Took out 'a' too\n",
        "  \n",
        "  ptrn_assert_en = r'\\b\\s*(the|be|and|of|to|have|it|that)\\s+'\n",
        "  \n",
        "  value = str()\n",
        "  length = len(args)\n",
        "  items = length-2\n",
        "  toggle_bit = [False]*items\n",
        "  limit = length-1\n",
        "  datapoint = args[limit-1]\n",
        "  idx = 0\n",
        "  \n",
        "  for n,arg in enumerate(args):\n",
        "    \n",
        "    if n == items:\n",
        "      break\n",
        "      \n",
        "    idx = args[limit].loc[datapoint,arg].find('en: ')\n",
        "   \n",
        "    if idx == -1:\n",
        "      value = ''\n",
        "      toggle_bit[n] = True\n",
        "    else: # en: exists\n",
        "      # is it truely english despite its label en: ? See datapoint 32 as e.g.\n",
        "      \n",
        "      found = [m for m in map(str.lower, \n",
        "               re.findall(ptrn_assert_en, args[limit].loc[datapoint,arg],\n",
        "               flags=re.IGNORECASE))]\n",
        "      \n",
        "      if len(found)!=0:\n",
        "        \n",
        "        itera = re.finditer(r\"\\.\\s+[a-z][a-z]:\", args[limit].loc[datapoint,arg])\n",
        "        indices = [m.start(0) for m in itera]\n",
        "        for indx in indices:\n",
        "          if idx < indx: \n",
        "            value = args[limit].loc[datapoint,arg][idx:indx] \n",
        "            # just run op once\n",
        "            break\n",
        "\n",
        "        else:    \n",
        "          value = args[limit].loc[datapoint,arg][idx:]\n",
        "      else:\n",
        "        # not in english despite label\n",
        "        value = ''\n",
        "        toggle_bit[n] = True\n",
        "  \n",
        "  return toggle_bit, value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbHN351holDk"
      },
      "source": [
        "def write_to_file(data, en_dataset_path):\n",
        "  \n",
        "  success = True\n",
        "  try: # data => <str, str, list, list>\n",
        "    with open(en_dataset_path, 'a') as en_dataset:\n",
        "        en_dataset.write(data[0] + '@' + data[1])\n",
        "        \n",
        "        en_dataset.write('@')\n",
        "        \n",
        "        for itr in range(len(data[2])):\n",
        "          en_dataset.write(data[2][itr] + ' ')\n",
        "        \n",
        "        en_dataset.write('@')\n",
        "        \n",
        "        for its in range(len(data[3])):  \n",
        "          en_dataset.write(data[3][its] + ' ')\n",
        "        \n",
        "        en_dataset.write('\\n')\n",
        "  except:\n",
        "    success = False \n",
        "  \n",
        "  data.clear()\n",
        "  return success"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw_zI1uP4ioV"
      },
      "source": [
        "def write_vocab_to_file(vocab_list, dest_file):\n",
        "  \n",
        "  success = True\n",
        "  try:\n",
        "    with open(dest_file, 'w') as vocab:\n",
        "        # Write RESERVED TOKENS expected by Tensor2Tensor\n",
        "        vocab.write('<pad>')\n",
        "        vocab.write('\\n')\n",
        "        vocab.write('<EOS>')\n",
        "        # other writes follow...\n",
        "        for token in vocab_list:\n",
        "          vocab.write('\\n' + token)\n",
        "  except:\n",
        "    success = False \n",
        "  \n",
        "  vocab_list.clear()\n",
        "  return success"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHMELILXrWI"
      },
      "source": [
        "def data_generator(en_dataset_path, split):\n",
        "    \n",
        "    df = pd.read_csv(en_dataset_path,sep='@',error_bad_lines=False, \n",
        "                     warn_bad_lines=False, header=0)   \n",
        "    number_of_columns = len(list(df.columns))\n",
        "    for i in range(int(df.size/number_of_columns)): #size = number of datapoints x (number of columns)\n",
        "      abstract = str(df.loc[i,'Abstract'])   \n",
        "      claims = str(df.loc[i,'Claims'])\n",
        "      split_str = u' <split> ' if split else \" \"\n",
        "      yield abstract + split_str + claims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdc2tfGhaG9k"
      },
      "source": [
        "def token_text_encoder(vocab_list, dest_filename):\n",
        "    \"\"\" Takes in UNIQUE vocab for encoding by TokenTextEncoder class \"\"\"\n",
        "    \n",
        "    from tensor2tensor.data_generators import text_encoder\n",
        "   \n",
        "    vocab_list.append(\"UNK\")\n",
        "    try:\n",
        "      te = text_encoder.TokenTextEncoder(vocab_filename=None,\n",
        "                 reverse=False,\n",
        "                 vocab_list=vocab_list,\n",
        "                 replace_oov=\"UNK\",   #Note: implementing backoff\n",
        "                 num_reserved_ids=2) # <bos> (or <pad>) and <eos>\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    try:\n",
        "      te.store_to_file(dest_filename)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRBtbrWxKPFf"
      },
      "source": [
        "def preprocess_data(find_dataset, en_dataset_path):\n",
        "\n",
        "  import time\n",
        "  import progressbar\n",
        "  \n",
        "  #Patterns\n",
        "  ptrn_title_en = re.compile(r'en:.*')\n",
        "  ptrn_abstract_en = re.compile(r'en:\\s*.*')\n",
        "  neg_ptrn_lead_xter = re.compile(r'en:\\s*')\n",
        "  neg_ptrn_slash_es = re.compile(r'\\b\\s+')\n",
        "  sentences = re.compile(r'\\b.*\\.')\n",
        "  extraneous = re.compile(r'(CLAIMS1|CLAIMS1.|claims1|claims1.)')\n",
        "  \n",
        "  csv_output_buffer = []\n",
        "  en_title = []\n",
        "  en_abstract = ['']\n",
        "  en_claims = ''\n",
        "  en_description = ''\n",
        "  lines_written = 0\n",
        "  \n",
        "  df = pd.read_csv(find_dataset,sep=';', error_bad_lines=False, \n",
        "                   warn_bad_lines=False, header=0)  \n",
        "  number_of_columns = len(list(df.columns))\n",
        "  datapoints = int(df.size/number_of_columns)\n",
        "\n",
        "  # Note: The following still falls through the crack:\n",
        "  #       1. Scenario where another language is embedded as lines along with\n",
        "  #          English. Example, datapoint 30\n",
        "  #       2. Abstracts are often too short to test for most common english words\n",
        "  \n",
        "  bar = progressbar.ProgressBar()\n",
        "  for i in bar(range(datapoints)):\n",
        "    \n",
        "    # Note : If all of Abstract, Claims or Description is not in English or\n",
        "    #        dont exist we will skip that datapoint \n",
        "    # Note: Some datapoints though tagged 'en:', aint in english; refer dp 99\n",
        "\n",
        "    toggle_bit_on_claims = False\n",
        "    toggle_bit_on_abstract = False\n",
        "    toggle_bit_on_desc = False\n",
        "    \n",
        "    try:\n",
        "      en_title = re.findall(ptrn_title_en, df.loc[i,'Title'])\n",
        "    except:\n",
        "      continue # in any event that 'en:' pattern  is not found \n",
        "    \n",
        "    # Handle multiple english titles\n",
        "    if len(en_title) > 1:\n",
        "      for k in range(len(en_title)-1):\n",
        "        en_title.pop(0)\n",
        "    # Handle title-less docs (even after en: pattern found) with a placeholder \n",
        "    if len(en_title) == 0:\n",
        "        en_title = ['LOREM_IPSUM']\n",
        "        \n",
        "    # Note: Some datapoints dont have either of abstracts, claims or description\n",
        "    try: \n",
        "      en_abstract = re.findall(ptrn_abstract_en, df.loc[i,'Abstract']) \n",
        "\n",
        "      # Handle multiple english abstracts e.g in datapoint 14 \n",
        "      if len(en_abstract) > 1:\n",
        "        for j in range(len(en_abstract)-1):\n",
        "          en_abstract.pop(0)\n",
        "\n",
        "      if len(en_abstract) == 0:\n",
        "        en_abstract = ['']\n",
        "        toggle_bit_on_abstract = True\n",
        "\n",
        "    except:  \n",
        "      # Also toggle-on when it dont exist or for some other unknown\n",
        "      toggle_bit_on_abstract = True\n",
        "      # Note: Template <COL1, COL2,...COLN, i, dataframe> for fxn extract_english\n",
        "    try:\n",
        "      toggler, en_claims = extract_english('Claims',i,df)\n",
        "      toggle_bit_on_claims = toggler[0]\n",
        "    except:\n",
        "      toggle_bit_on_claims = True\n",
        "    try:  \n",
        "      toggler, en_description = extract_english('Description',i,df)\n",
        "      toggle_bit_on_desc = toggler[0]\n",
        "    except:\n",
        "      toggle_bit_on_desc = True\n",
        "\n",
        "    # Test for fidelity: No point in retaining datapoint if one of the following\n",
        "    # is in another language.\n",
        "    if toggle_bit_on_claims and toggle_bit_on_desc:\n",
        "      continue\n",
        "\n",
        "    ## Further data processing\n",
        "    if len(en_title) != 0:\n",
        "      en_title[0] = neg_ptrn_lead_xter.sub('', en_title[0])\n",
        "    if len(en_abstract) != 0:\n",
        "      en_abstract[0] = neg_ptrn_lead_xter.sub('', en_abstract[0])\n",
        "    if en_claims is not '':\n",
        "      en_claims = neg_ptrn_lead_xter.sub('', en_claims)\n",
        "    if en_description is not '':\n",
        "      en_description = neg_ptrn_lead_xter.sub('', en_description)\n",
        "    \n",
        "    # Take out hidden spaces, tabs etc and break-off into sentences\n",
        "    copy_en_desc = neg_ptrn_slash_es.sub(' ', en_description)\n",
        "    copy2_en_desc = [''.join(itm) for itm in re.findall(sentences, copy_en_desc)]   \n",
        "    \n",
        "    copy_en_claims = neg_ptrn_slash_es.sub(' ', en_claims)  \n",
        "    copy1_en_claims = extraneous.sub(' ', copy_en_claims)\n",
        "    copy2_en_claims = [''.join(itm) for itm in re.findall(sentences, copy1_en_claims)]\n",
        "    \n",
        "    if len(copy2_en_desc) == 0 and len(copy2_en_claims) == 0:\n",
        "      data = [en_title[0], en_abstract[0], [' '], [' ']]\n",
        "    elif len(copy2_en_claims) == 0 and len(copy2_en_desc) != 0:\n",
        "      data = [en_title[0], en_abstract[0], [' '], copy2_en_desc[:]]\n",
        "    elif len(copy2_en_desc) == 0 and len(copy2_en_claims) != 0:\n",
        "      data = [en_title[0], en_abstract[0], copy2_en_claims[:], [' ']]\n",
        "    else:\n",
        "      data = [en_title[0], en_abstract[0], copy2_en_claims[:], copy2_en_desc[:]]\n",
        "    \n",
        "    success = write_to_file(data, en_dataset_path)\n",
        "    if success:\n",
        "      lines_written = lines_written + 1\n",
        "    else:\n",
        "      exit(\"Failed to write file: {}; Program exiting.\".format(en_dataset_path))\n",
        "    \n",
        "  return lines_written"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lcgJysWS9ct"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZDQZ6898M4m"
      },
      "source": [
        "** \\__init__  ** : Indicate to T2T there's a custom problem to load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwhsHLNWRk0o",
        "outputId": "d1e49b19-d0b3-4e6d-9e60-e27d4704fa10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile __init__.py\n",
        "from . import registrar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing __init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2k-TyIn_m6M"
      },
      "source": [
        "**registrar** - register summarization problem instance with tensor2tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OtQAZA_9zJt",
        "outputId": "e1c98efe-a210-4111-fb3c-244e8e1d3b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile registrar.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensor2tensor.data_generators import generator_utils\n",
        "from tensor2tensor.data_generators import problem\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "from tensor2tensor.data_generators import text_problems\n",
        "# from tensor2tensor.data_generators import wiki_lm\n",
        "from tensor2tensor.utils import registry\n",
        "import os\n",
        "\n",
        "def splitter(document):\n",
        "  \n",
        "    split_str = u' <split> '\n",
        "    split_str_len = len(split_str)\n",
        "    split_pos = document.find(split_str)\n",
        "    return document[:split_pos], document[split_pos + split_str_len:]  # abstract, claims\n",
        "\n",
        "def example_generator(split):\n",
        "    \n",
        "    import pandas as pd\n",
        "    \n",
        "    # TODO : pass the dataset location in by reading from a file locally prior\n",
        "    #        written to by main()\n",
        "    \n",
        "    en_dataset_path = '/content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_6978_dp_clean.csv'\n",
        "    #en_dataset_path = '/content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_1944_dp_clean.csv'\n",
        "    \n",
        "    df = pd.read_csv(en_dataset_path,sep='@',error_bad_lines=False, \n",
        "                     warn_bad_lines=False, header=0)\n",
        "    number_of_columns = len(list(df.columns))\n",
        "    for i in range(int(df.size/number_of_columns)): #size = number of datapoints x (number of columns)\n",
        "      abstract = str(df.loc[i,'Abstract'])   \n",
        "      claims = str(df.loc[i,'Claims'])\n",
        "      split_str = u' <split> ' if split else \" \"\n",
        "      yield abstract + split_str + claims\n",
        "\n",
        "@registry.register_problem('summarize_patent')\n",
        "class SummarizePatent(text_problems.Text2TextProblem):\n",
        "\n",
        "  def write_raw_text_to_files(self, dataset_split, tmp_dir):\n",
        "\n",
        "    def write_to_file(tmp_dir, filename):\n",
        "      \"\"\"Write text to files.\"\"\"\n",
        "      with open(os.path.join(tmp_dir, filename + \".source\"), \"w\") as fstory:\n",
        "        with open(os.path.join(tmp_dir, filename + \".target\"), \"w\") as fsummary:\n",
        "          for document in example_generator(True): \n",
        "            abstract, claims = splitter(document)\n",
        "            fstory.write(claims + \"\\n\")\n",
        "            fsummary.write(abstract + \"\\n\")\n",
        "   \n",
        "    if dataset_split == problem.DatasetSplit.TRAIN:\n",
        "      filename = \"patentsumm.train\"\n",
        "    elif dataset_split == problem.DatasetSplit.EVAL:\n",
        "      filename = \"patentsumm.dev\"\n",
        "    else:\n",
        "      filename = \"patentsumm.test\"\n",
        "    \n",
        "    write_to_file(tmp_dir, filename)\n",
        " \n",
        "  # Note: The following functions are the least 2 that must be overriden\n",
        "  #       of text_problems. Refer /data_generators/text_problems.py \n",
        "  \n",
        "  def is_generate_per_split(self):              \n",
        "    return False # do auto split and base it on ratio defined by dataset_split\n",
        "  \n",
        "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
        "    \n",
        "    del data_dir\n",
        "    self.write_raw_text_to_files(dataset_split, tmp_dir)\n",
        "    split = True\n",
        "    for document in example_generator(split):\n",
        "      abstract, claims = splitter(document)\n",
        "      yield {\"inputs\": claims, \"targets\": abstract}\n",
        "  \n",
        "  ## Other overrides\n",
        "  \n",
        "  def generate_text_for_vocab(self, data_dir, tmp_dir):\n",
        "    del data_dir\n",
        "    del tmp_dir\n",
        "    split = False\n",
        "    return example_generator(split)\n",
        "  \n",
        "  # Modifiable properties\n",
        "  \n",
        "  @property\n",
        "  def vocab_type(self):\n",
        "    return text_problems.VocabType.TOKEN   # refer text_problems.py\n",
        "  \n",
        "  \n",
        "  @property\n",
        "  def dataset_splits(self):\n",
        "    \"\"\" Splits of data to produce and number of output shards for each. \"\"\"\n",
        "    return [{\n",
        "        \"split\": problem.DatasetSplit.TRAIN,\n",
        "        \"shards\": 100,\n",
        "    }, {\n",
        "        \"split\": problem.DatasetSplit.EVAL,\n",
        "        \"shards\": 10,\n",
        "    }, {\n",
        "        \"split\": problem.DatasetSplit.TEST,\n",
        "        \"shards\": 10,\n",
        "    }]\n",
        "  \n",
        "  @property\n",
        "  def oov_token(self):\n",
        "    \"\"\"Out of vocabulary token. Only for VocabType.TOKEN.\"\"\"\n",
        "    return \"UNK\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing registrar.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhAn0GyEcPN1"
      },
      "source": [
        "**data_processing**: Preprocess the data and build vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNlWYWJ5TCQ4",
        "outputId": "b07b6313-7476-4a16-dcf5-f9541ad434bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile data_processing.py\n",
        "\n",
        "import os\n",
        "\n",
        "def processor(find_dataset, en_dataset_path):\n",
        "  \n",
        "  if not os.path.exists(en_dataset_path):    \n",
        "\n",
        "    # Touch en_dataset\n",
        "    success = touch(en_dataset_path)\n",
        "    if not success:\n",
        "      exit(\"Problem creating file: {}; Program exiting.\".format(en_dataset_path))\n",
        "\n",
        "    print(\"File created: {}\".format(en_dataset_path), '\\n')\n",
        "\n",
        "    lines_written = preprocess_data(find_dataset, en_dataset_path)\n",
        "\n",
        "    print('Total Number of Lines Written to {} : '.format(en_dataset_path), \n",
        "          lines_written)\n",
        "  else:\n",
        "    print(\"File already exists: {}\".format(en_dataset_path), '\\n')\n",
        "  \n",
        "  return"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing data_processing.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHGTp_OFnW_o"
      },
      "source": [
        "**vocabulary**: Build vocabulary locally from clean english dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkSpV8hYdcct",
        "outputId": "1504d084-c764-4a1f-97c4-4e954df92027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile vocabulary.py\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "END_TOKENS = [u'?', u'.', u'!', u',', u'\\'', u'\"',u\"`\", u\")\"]\n",
        "\n",
        "# TODO: Import utility.py to gain acess to utility functions used here\n",
        "#from . import utility \n",
        "\n",
        "def create_vocab(en_dataset_path, save_to_dir):\n",
        "  \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  # prep to remove punctutaions \n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  \n",
        "  feedback = data_generator(en_dataset_path, False)\n",
        "  \n",
        "  #TODO: remove instances of NAN during that seep into the data during pre-processing \n",
        "\n",
        "  #QUERY: LEMATIZATION?\n",
        "  count = 0\n",
        "  tokens = []\n",
        "  vocab_list = []\n",
        "  for item in feedback:\n",
        "    for token in word_tokenize(item.lower().strip()):\n",
        "      count = count + 1  \n",
        "      if token not in stop_words:\n",
        "        if token not in END_TOKENS and token.isalnum():  #### OR ISALPHA?\n",
        "          tokens.append(token)\n",
        "\n",
        "  depunctuated_tokens = [token.translate(table) for token in tokens]        \n",
        "  # remove duplicates, if any\n",
        "  tokens.clear()\n",
        "  tokens = list(dict.fromkeys(depunctuated_tokens)) \n",
        "  vocab_list.extend(tokens)\n",
        "  # append UNK for back-off: vocab requirement imposed by tensor2tensor\n",
        "  vocab_list.append(\"UNK\")\n",
        "\n",
        "  # Note: Theres a format to be followed in naming vocab file for T2T problems\n",
        "  success = write_vocab_to_file(vocab_list, save_to_dir + u'/vocab.summarize_patent.tokens')\n",
        "  \n",
        "  if success:\n",
        "    print(\"INFO: Vocabulary successfully written to: {}; \".format(\n",
        "      save_to_dir + u'/vocab.summarize_patent.tokens'))\n",
        "  else:\n",
        "    print(\"INFO: Failed to write vocabulary to: {}; \".format(\n",
        "      save_to_dir + u'/vocab.summarize_patent.tokens'))\n",
        "    return sys.exit('ERROR: Program exiting.')\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing vocabulary.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVPlkodK9oGE"
      },
      "source": [
        "**cloud_download** : Copy cloud/gdrive file(s) locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjOTJYCeUuC4",
        "outputId": "923c1146-268c-4aab-9ffb-ea3ff32927ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile clouddownload.py\n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def copy_cloud_file_locally(*args):\n",
        "  \"\"\" Download file(s) from cloud/drive to local_path \"\"\"\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  \n",
        "  local_path, filenames = args\n",
        "  \n",
        "  if filenames is None:\n",
        "    return \"Error: You must supply a filename.\"\n",
        "  \n",
        "  # choose a local (colab) directory to store the data.\n",
        "  local_download_path = os.path.expanduser(local_path)\n",
        "  \n",
        "  for filename in filenames:\n",
        "    try:\n",
        "      # TODO: Handle case where a replica of a file possibly existing in Trash...\n",
        "      listed = drive.ListFile({'q':\"title contains \" + \"'\" + filename + \"'\", 'spaces':'drive'}).GetList()\n",
        "      for file in listed:\n",
        "        fname = os.path.join(local_download_path, file['title'])\n",
        "        print('INFO: Downloading to {}'.format(fname))\n",
        "        f_ = drive.CreateFile({'id': file['id']})\n",
        "        f_.GetContentFile(fname)\n",
        "    except:\n",
        "      return \"Error: File not found in source: cloud/drive.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing clouddownload.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4mwhI5xCm_f"
      },
      "source": [
        "**\\__main__**: Copy cloud/gdrive file(s) locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGVIQOfAV8Lb",
        "outputId": "a12698ec-1ec3-46ba-a3ed-8b4bacf5989d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10657
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "#Enable TF Eager execution\n",
        "tfe = tf.contrib.eager\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.layers import common_layers\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "from tensor2tensor.utils import t2t_model\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import metrics\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "\n",
        "#from . import clouddownload \n",
        "#from . import data_processing\n",
        "#from . import vocabulary\n",
        "exec(compile(open('clouddownload.py', \"rb\").read(), 'clouddownload.py', 'exec'))\n",
        "exec(compile(open('data_processing.py', \"rb\").read(), 'data_processing.py', 'exec'))\n",
        "exec(compile(open('vocabulary.py', \"rb\").read(), 'vocabulary.py', 'exec'))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "FLAGS = None\n",
        "DEFAULT_FILES_DDIR = ['sample_101010_dp_clean.csv']\n",
        "\n",
        "#ptrn_path = re.compile(r'\\/.*\\/')\n",
        "ptrn_dp = re.compile(r'_\\d*_')\n",
        "\n",
        "def main():\n",
        "  \n",
        "    # TODO Handle where all flags empty\n",
        "   \n",
        "  t2t_usr_dir = FLAGS.t2t_usr_dir  \n",
        "  data_dir = FLAGS.data_dir\n",
        "  tmp_dir = FLAGS.tmp_dir\n",
        "  train_dir = FLAGS.train_dir\n",
        "  problem = FLAGS.problem\n",
        "  more_cloud_files_to_download = FLAGS.download_cloud_files\n",
        "  \n",
        "    # TODO: Consider using os.makedir() ...\n",
        "  if not os.path.exists(data_dir):\n",
        "    tf.gfile.MakeDirs(data_dir)\n",
        "  if not os.path.exists(tmp_dir):\n",
        "    tf.gfile.MakeDirs(tmp_dir)\n",
        "  if not os.path.exists(train_dir):\n",
        "    tf.gfile.MakeDirs(train_dir)\n",
        "  \n",
        "  # data-(pre)processing call\n",
        "  \n",
        "    #TODO : Read the raw dataset location in cloud in with flag\n",
        "  find_dataset = u'/content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_6978_dp.csv'\n",
        "  #find_dataset = u'/content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_1944_dp.csv'\n",
        "  \n",
        "  dp = re.findall(ptrn_dp, find_dataset).pop(0)\n",
        "  dp = int(dp.lstrip('_').rstrip('_'))\n",
        "  \n",
        "  data_dir_path_in_cloud, _ = os.path.split(find_dataset)\n",
        "  en_dataset_path = data_dir_path_in_cloud + u'/' + 'sample_{}_dp_clean.csv'.format(dp)\n",
        "  processor(find_dataset, en_dataset_path)\n",
        "  \n",
        "  # create vocab (locally) call\n",
        "  create_vocab(en_dataset_path, data_dir)\n",
        "    \n",
        "  # Add the clean english dataset to list of files to download from cloud\n",
        "  add_file = 'sample_{}_dp_clean.csv'.format(dp)\n",
        "  DEFAULT_FILES_DDIR.append(add_file)\n",
        "  files_to_dload_to_ddir = DEFAULT_FILES_DDIR\n",
        "  \n",
        "  if not FLAGS.download_cloud_files is None:\n",
        "    # Assuming they're in .../data_dir in CLOUD\n",
        "    add_default_files = [str.strip(fl) for fl in more_cloud_files_to_download.split(',')]    \n",
        "    files_to_dload_to_ddir.extend(add_default_files)\n",
        "    copy_cloud_file_locally(data_dir, files_to_dload_to_ddir)\n",
        "  \n",
        "  # ...where there's no additional files to download locally to --data_dir\n",
        "  copy_cloud_file_locally(data_dir, files_to_dload_to_ddir)\n",
        "  \n",
        "  #TODO: Remove eventually; in the mean time, simulating CMD-line\n",
        "  #!t2t-datagen --t2t_usr_dir='/content' --data_dir='/content/data_dir' --tmp_dir='/content/tmp_dir' --problem=summarize_patent\n",
        "  !t2t-datagen --t2t_usr_dir='/content' --data_dir='/content/data_dir' --tmp_dir='/content/tmp_dir' --problem=summarize_patent\n",
        "  \n",
        "  # TODO: code to commence training should come here\n",
        "  !t2t-trainer --data_dir='/content/data_dir' --problem=summarize_patent \\\n",
        "    --model=transformer --hparams_set=transformer_base_single_gpu \\\n",
        "    --output_dir='/content/train_dir' \\\n",
        "    --hparams='batch_size=1024' \\\n",
        "    --t2t_usr_dir='/content' \\\n",
        "    --train_steps=140000\n",
        "  \n",
        "  # TODO: code to infer should come here; perhaps a filename is passed to main\n",
        "  # where data can be read from\n",
        "  \n",
        "  ## Decode; NOTE: Toggle --decode_from_file to use eval/test set rather\n",
        "  #--decode_from_file='/content/data_dir/decode_data_3560.txt' \\\n",
        "  \n",
        "  !t2t-decoder --data_dir='/content/data_dir' --problem=summarize_patent \\\n",
        "  --model=transformer --hparams_set=transformer_base_single_gpu \\\n",
        "  --output_dir='/content/train_dir' --decode_hparams=\"beam_size=4,alpha=0.6\" \\\n",
        "  --t2t_usr_dir='/content' \\\n",
        "  --decode_to_file=summarized_decoded.txt\n",
        "  \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "  #main()\n",
        "  \n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--t2t_usr_dir',\n",
        "      type=str,\n",
        "      default='/content',\n",
        "      help='Location of __init__ menat to import user components.')\n",
        "  parser.add_argument(\n",
        "      '--data_dir',\n",
        "      type=str,\n",
        "      default='/content/data_dir',\n",
        "      help=\"\"\"\\\n",
        "      Where to locate data files and vocabulary file.\n",
        "      \"\"\")\n",
        "  parser.add_argument(\n",
        "      '--tmp_dir',\n",
        "      type=str,\n",
        "      default='/content/tmp_dir',                                                                      \n",
        "      help=\"\"\"\\\n",
        "      Where to locate any other related files e.g. *.source and *.target files.\n",
        "      \"\"\")\n",
        "  parser.add_argument(\n",
        "      '--train_dir',\n",
        "      type=str,\n",
        "      default='/content/train_dir',                                                                      \n",
        "      help=\"\"\"\\\n",
        "      Output of training process will be put here.\n",
        "      \"\"\")\n",
        "  parser.add_argument(\n",
        "      '--download_cloud_files',\n",
        "      type=str,\n",
        "      default=None,                                                                      \n",
        "      help=\"\"\"\\\n",
        "      Comma seperated list of filenames to download form cloud data_dir folder.\n",
        "      \"\"\")\n",
        "  \n",
        "  parser.add_argument(\n",
        "      '--problem',\n",
        "      type=str,\n",
        "      default='summarize_patent',\n",
        "      help=\"\"\"\\\n",
        "      Snake-cased name of the problem definition.\n",
        "      \"\"\")\n",
        "   \n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "  main()\n",
        "  #tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Entry Point [tensor2tensor.envs.tic_tac_toe_env:TicTacToeEnv] registered with id [T2TEnv-TicTacToeEnv-v0]\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "File already exists: /content/drive/My Drive/Thesis_at_IamIP/data_dir/sample_6978_dp_clean.csv \n",
            "\n",
            "INFO: Vocabulary successfully written to: /content/data_dir/vocab.summarize_patent.tokens; \n",
            "INFO: Downloading to /content/data_dir/sample_101010_dp_clean.csv\n",
            "INFO: Downloading to /content/data_dir/sample_6978_dp_clean.csv\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Importing user module content from path /\n",
            "INFO:tensorflow:Generating problems:\n",
            "    summarize:\n",
            "      * summarize_patent\n",
            "INFO:tensorflow:Generating data for summarize_patent.\n",
            "INFO:tensorflow:Generating case 0.\n",
            "INFO:tensorflow:Generated 6965 Examples\n",
            "INFO:tensorflow:Generating case 0.\n",
            "INFO:tensorflow:Generated 6965 Examples\n",
            "INFO:tensorflow:Generating case 0.\n",
            "INFO:tensorflow:Generated 6965 Examples\n",
            "INFO:tensorflow:Shuffling data...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/data_generators/generator_utils.py:469: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:tensorflow:Data shuffled.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Importing user module content from path /\n",
            "INFO:tensorflow:Overriding hparams in transformer_base_single_gpu with batch_size=1024\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/trainer_lib.py:240: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
            "INFO:tensorflow:Configuring DataParallelism to replicate the model.\n",
            "INFO:tensorflow:schedule=continuous_train_and_eval\n",
            "INFO:tensorflow:worker_gpu=1\n",
            "INFO:tensorflow:sync=False\n",
            "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
            "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
            "INFO:tensorflow:caching_devices: None\n",
            "INFO:tensorflow:ps_devices: ['gpu:0']\n",
            "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa14937d0b8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_eval_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 1.0\n",
            "}\n",
            ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_protocol': None, '_session_config': gpu_options {\n",
            "  per_process_gpu_memory_fraction: 0.95\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  optimizer_options {\n",
            "    global_jit_level: OFF\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/content/train_dir', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fa14937d128>}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fa1493658c8>) includes params argument, but params are not passed to Estimator.\n",
            "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-train*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:275: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:37: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/data_reader.py:233: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py:1007: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/models/transformer.py:93: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "INFO:tensorflow:Base learning rate: 0.100000\n",
            "INFO:tensorflow:Trainable Variables Total size: 59796992\n",
            "INFO:tensorflow:Non-trainable variables Total size: 5\n",
            "INFO:tensorflow:Using optimizer adam\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 21:15:27.008789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-07 21:15:27.009105: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x30fac00 executing computations on platform Host. Devices:\n",
            "2019-06-07 21:15:27.009139: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-07 21:15:27.227320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-07 21:15:27.227826: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x30fa940 executing computations on platform CUDA. Devices:\n",
            "2019-06-07 21:15:27.227855: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-07 21:15:27.228188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-06-07 21:15:27.228216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 21:15:28.583533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 21:15:28.583601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 21:15:28.583620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 21:15:28.583872: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-07 21:15:28.583947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /content/train_dir/model.ckpt.\n",
            "2019-06-07 21:15:56.910202: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "INFO:tensorflow:loss = 8.753455, step = 0\n",
            "INFO:tensorflow:global_step/sec: 4.12497\n",
            "INFO:tensorflow:loss = 5.318731, step = 100 (24.242 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.92503\n",
            "INFO:tensorflow:loss = 4.4553094, step = 200 (16.878 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88456\n",
            "INFO:tensorflow:loss = 4.210534, step = 300 (16.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90172\n",
            "INFO:tensorflow:loss = 4.182724, step = 400 (16.944 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86823\n",
            "INFO:tensorflow:loss = 3.624872, step = 500 (17.041 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.79908\n",
            "INFO:tensorflow:loss = 4.0432215, step = 600 (17.245 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.96746\n",
            "INFO:tensorflow:loss = 3.5421255, step = 700 (16.757 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87918\n",
            "INFO:tensorflow:loss = 3.6304853, step = 800 (17.009 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8687\n",
            "INFO:tensorflow:loss = 3.401633, step = 900 (17.041 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/bleu_hook.py:151: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-07T21:19:12Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 21:19:13.697243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 21:19:13.697316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 21:19:13.697334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 21:19:13.697359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 21:19:13.697575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/train_dir/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-07-21:22:53\n",
            "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 4.23247, metrics-summarize_patent/targets/accuracy = 0.51580226, metrics-summarize_patent/targets/accuracy_per_sequence = 0.0, metrics-summarize_patent/targets/accuracy_top5 = 0.5603974, metrics-summarize_patent/targets/approx_bleu_score = 0.08904128, metrics-summarize_patent/targets/neg_log_perplexity = -4.23804, metrics-summarize_patent/targets/rouge_2_fscore = 0.047478866, metrics-summarize_patent/targets/rouge_L_fscore = 0.33989677\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /content/train_dir/model.ckpt-1000\n",
            "INFO:tensorflow:global_step/sec: 0.39863\n",
            "INFO:tensorflow:loss = 3.126936, step = 1000 (250.857 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82765\n",
            "INFO:tensorflow:loss = 3.4509947, step = 1100 (17.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82361\n",
            "INFO:tensorflow:loss = 3.6812727, step = 1200 (17.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93905\n",
            "INFO:tensorflow:loss = 3.6003935, step = 1300 (16.837 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82887\n",
            "INFO:tensorflow:loss = 3.3699102, step = 1400 (17.156 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82369\n",
            "INFO:tensorflow:loss = 3.2148175, step = 1500 (17.172 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86374\n",
            "INFO:tensorflow:loss = 3.4127185, step = 1600 (17.054 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.92051\n",
            "INFO:tensorflow:loss = 3.5009997, step = 1700 (16.890 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93494\n",
            "INFO:tensorflow:loss = 3.5297394, step = 1800 (16.850 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.99032\n",
            "INFO:tensorflow:loss = 2.803258, step = 1900 (16.693 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.46939\n",
            "INFO:tensorflow:loss = 3.0581284, step = 2000 (22.374 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87216\n",
            "INFO:tensorflow:loss = 2.9670184, step = 2100 (17.030 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.99548\n",
            "INFO:tensorflow:loss = 2.7304704, step = 2200 (16.679 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8246\n",
            "INFO:tensorflow:loss = 2.9388, step = 2300 (17.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84304\n",
            "INFO:tensorflow:loss = 2.7714458, step = 2400 (17.114 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86217\n",
            "INFO:tensorflow:loss = 2.8352847, step = 2500 (17.059 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.95064\n",
            "INFO:tensorflow:loss = 2.304131, step = 2600 (16.806 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.81593\n",
            "INFO:tensorflow:loss = 2.7652953, step = 2700 (17.194 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93896\n",
            "INFO:tensorflow:loss = 2.3674302, step = 2800 (16.839 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91165\n",
            "INFO:tensorflow:loss = 2.5819118, step = 2900 (16.914 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.48684\n",
            "INFO:tensorflow:loss = 2.5898352, step = 3000 (22.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85699\n",
            "INFO:tensorflow:loss = 2.1643174, step = 3100 (17.075 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84726\n",
            "INFO:tensorflow:loss = 2.0396886, step = 3200 (17.103 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8683\n",
            "INFO:tensorflow:loss = 2.0222242, step = 3300 (17.040 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.99885\n",
            "INFO:tensorflow:loss = 1.7631723, step = 3400 (16.670 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86911\n",
            "INFO:tensorflow:loss = 2.3483937, step = 3500 (17.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85742\n",
            "INFO:tensorflow:loss = 1.6199571, step = 3600 (17.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91735\n",
            "INFO:tensorflow:loss = 1.6159295, step = 3700 (16.899 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8856\n",
            "INFO:tensorflow:loss = 1.8927302, step = 3800 (16.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89419\n",
            "INFO:tensorflow:loss = 1.9684178, step = 3900 (16.966 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-07T21:31:46Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 21:31:47.254532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 21:31:47.254609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 21:31:47.254625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 21:31:47.254635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 21:31:47.254820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /content/train_dir/model.ckpt-4000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-07-21:35:28\n",
            "INFO:tensorflow:Saving dict for global step 4000: global_step = 4000, loss = 3.9787936, metrics-summarize_patent/targets/accuracy = 0.49640456, metrics-summarize_patent/targets/accuracy_per_sequence = 0.0, metrics-summarize_patent/targets/accuracy_top5 = 0.61212456, metrics-summarize_patent/targets/approx_bleu_score = 0.1157071, metrics-summarize_patent/targets/neg_log_perplexity = -4.000293, metrics-summarize_patent/targets/rouge_2_fscore = 0.12249899, metrics-summarize_patent/targets/rouge_L_fscore = 0.36097553\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 4000: /content/train_dir/model.ckpt-4000\n",
            "INFO:tensorflow:global_step/sec: 0.396207\n",
            "INFO:tensorflow:loss = 1.5836725, step = 4000 (252.394 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82652\n",
            "INFO:tensorflow:loss = 1.7060585, step = 4100 (17.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90889\n",
            "INFO:tensorflow:loss = 1.7401801, step = 4200 (16.923 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.916\n",
            "INFO:tensorflow:loss = 1.6722918, step = 4300 (16.903 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91754\n",
            "INFO:tensorflow:loss = 1.829607, step = 4400 (16.899 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.99199\n",
            "INFO:tensorflow:loss = 1.4483968, step = 4500 (16.689 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91202\n",
            "INFO:tensorflow:loss = 1.3394169, step = 4600 (16.915 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87269\n",
            "INFO:tensorflow:loss = 1.5490569, step = 4700 (17.027 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85049\n",
            "INFO:tensorflow:loss = 1.5995728, step = 4800 (17.093 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.79907\n",
            "INFO:tensorflow:loss = 1.7093464, step = 4900 (17.245 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.36495\n",
            "INFO:tensorflow:loss = 1.4662299, step = 5000 (22.909 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.95562\n",
            "INFO:tensorflow:loss = 1.2465975, step = 5100 (16.791 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85214\n",
            "INFO:tensorflow:loss = 1.2557722, step = 5200 (17.087 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.95714\n",
            "INFO:tensorflow:loss = 1.3645666, step = 5300 (16.786 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84974\n",
            "INFO:tensorflow:loss = 1.3558955, step = 5400 (17.096 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.92306\n",
            "INFO:tensorflow:loss = 1.2532613, step = 5500 (16.883 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88007\n",
            "INFO:tensorflow:loss = 1.2812296, step = 5600 (17.006 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93365\n",
            "INFO:tensorflow:loss = 1.1585096, step = 5700 (16.853 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89901\n",
            "INFO:tensorflow:loss = 1.4365789, step = 5800 (16.951 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82441\n",
            "INFO:tensorflow:loss = 1.2084059, step = 5900 (17.169 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 6000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.40984\n",
            "INFO:tensorflow:loss = 1.3181922, step = 6000 (22.675 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88384\n",
            "INFO:tensorflow:loss = 1.2388501, step = 6100 (16.997 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8409\n",
            "INFO:tensorflow:loss = 1.3147353, step = 6200 (17.121 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.83768\n",
            "INFO:tensorflow:loss = 1.1563239, step = 6300 (17.128 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.06681\n",
            "INFO:tensorflow:loss = 1.1803359, step = 6400 (16.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93145\n",
            "INFO:tensorflow:loss = 1.1260475, step = 6500 (16.860 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90602\n",
            "INFO:tensorflow:loss = 1.2607851, step = 6600 (16.932 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.9105\n",
            "INFO:tensorflow:loss = 1.1230665, step = 6700 (16.920 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82076\n",
            "INFO:tensorflow:loss = 0.9180358, step = 6800 (17.178 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89785\n",
            "INFO:tensorflow:loss = 1.1945182, step = 6900 (16.956 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 7000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-07T21:44:22Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 21:44:23.468724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 21:44:23.468807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 21:44:23.468832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 21:44:23.468843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 21:44:23.468977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /content/train_dir/model.ckpt-7000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-07-21:47:52\n",
            "INFO:tensorflow:Saving dict for global step 7000: global_step = 7000, loss = 4.4227967, metrics-summarize_patent/targets/accuracy = 0.45774633, metrics-summarize_patent/targets/accuracy_per_sequence = 0.0, metrics-summarize_patent/targets/accuracy_top5 = 0.60538435, metrics-summarize_patent/targets/approx_bleu_score = 0.12557362, metrics-summarize_patent/targets/neg_log_perplexity = -4.46124, metrics-summarize_patent/targets/rouge_2_fscore = 0.15477389, metrics-summarize_patent/targets/rouge_L_fscore = 0.365927\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 7000: /content/train_dir/model.ckpt-7000\n",
            "INFO:tensorflow:global_step/sec: 0.416098\n",
            "INFO:tensorflow:loss = 1.1376178, step = 7000 (240.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86168\n",
            "INFO:tensorflow:loss = 1.0052285, step = 7100 (17.060 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88069\n",
            "INFO:tensorflow:loss = 1.5006018, step = 7200 (17.006 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.852\n",
            "INFO:tensorflow:loss = 1.2017088, step = 7300 (17.087 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90651\n",
            "INFO:tensorflow:loss = 1.2871583, step = 7400 (16.930 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91325\n",
            "INFO:tensorflow:loss = 0.917387, step = 7500 (16.911 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.96158\n",
            "INFO:tensorflow:loss = 1.0839349, step = 7600 (16.774 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.97008\n",
            "INFO:tensorflow:loss = 1.1017699, step = 7700 (16.750 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.83777\n",
            "INFO:tensorflow:loss = 1.1537133, step = 7800 (17.131 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8275\n",
            "INFO:tensorflow:loss = 0.9729196, step = 7900 (17.159 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 8000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.38876\n",
            "INFO:tensorflow:loss = 1.0070053, step = 8000 (22.785 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91443\n",
            "INFO:tensorflow:loss = 1.0281602, step = 8100 (16.910 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87375\n",
            "INFO:tensorflow:loss = 0.8354799, step = 8200 (17.024 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84206\n",
            "INFO:tensorflow:loss = 0.8490583, step = 8300 (17.117 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84564\n",
            "INFO:tensorflow:loss = 0.8017268, step = 8400 (17.107 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.92054\n",
            "INFO:tensorflow:loss = 0.8776002, step = 8500 (16.890 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89186\n",
            "INFO:tensorflow:loss = 0.8238648, step = 8600 (16.972 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90858\n",
            "INFO:tensorflow:loss = 0.8407477, step = 8700 (16.924 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89362\n",
            "INFO:tensorflow:loss = 0.8920368, step = 8800 (16.968 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88094\n",
            "INFO:tensorflow:loss = 0.69470185, step = 8900 (17.004 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 9000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.39067\n",
            "INFO:tensorflow:loss = 0.934563, step = 9000 (22.774 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.92193\n",
            "INFO:tensorflow:loss = 0.98082554, step = 9100 (16.888 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91302\n",
            "INFO:tensorflow:loss = 0.8198232, step = 9200 (16.911 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88335\n",
            "INFO:tensorflow:loss = 0.77999556, step = 9300 (16.997 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91546\n",
            "INFO:tensorflow:loss = 1.0102603, step = 9400 (16.905 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87925\n",
            "INFO:tensorflow:loss = 0.8764172, step = 9500 (17.010 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93052\n",
            "INFO:tensorflow:loss = 1.3459638, step = 9600 (16.862 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.93677\n",
            "INFO:tensorflow:loss = 0.79371226, step = 9700 (16.843 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.77714\n",
            "INFO:tensorflow:loss = 0.90222275, step = 9800 (17.311 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85835\n",
            "INFO:tensorflow:loss = 0.6938117, step = 9900 (17.070 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-07T21:56:46Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 21:56:47.266224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 21:56:47.266307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 21:56:47.266324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 21:56:47.266337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 21:56:47.266483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /content/train_dir/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n",
            "INFO:tensorflow:Evaluation [80/100]\n",
            "INFO:tensorflow:Evaluation [90/100]\n",
            "INFO:tensorflow:Evaluation [100/100]\n",
            "INFO:tensorflow:Finished evaluation at 2019-06-07-22:00:11\n",
            "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 4.5202065, metrics-summarize_patent/targets/accuracy = 0.49283853, metrics-summarize_patent/targets/accuracy_per_sequence = 0.00990099, metrics-summarize_patent/targets/accuracy_top5 = 0.60397357, metrics-summarize_patent/targets/approx_bleu_score = 0.14015302, metrics-summarize_patent/targets/neg_log_perplexity = -4.5586047, metrics-summarize_patent/targets/rouge_2_fscore = 0.17703545, metrics-summarize_patent/targets/rouge_L_fscore = 0.37251475\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: /content/train_dir/model.ckpt-10000\n",
            "INFO:tensorflow:global_step/sec: 0.42636\n",
            "INFO:tensorflow:loss = 0.74187917, step = 10000 (234.556 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82025\n",
            "INFO:tensorflow:loss = 0.5965896, step = 10100 (17.168 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.94266\n",
            "INFO:tensorflow:loss = 0.7221077, step = 10200 (16.827 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89033\n",
            "INFO:tensorflow:loss = 0.91340196, step = 10300 (16.977 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.9212\n",
            "INFO:tensorflow:loss = 0.75310504, step = 10400 (16.889 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86117\n",
            "INFO:tensorflow:loss = 1.0893396, step = 10500 (17.060 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85311\n",
            "INFO:tensorflow:loss = 0.67269844, step = 10600 (17.085 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.01685\n",
            "INFO:tensorflow:loss = 0.5185415, step = 10700 (16.620 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.95388\n",
            "INFO:tensorflow:loss = 0.6414366, step = 10800 (16.795 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8351\n",
            "INFO:tensorflow:loss = 0.5036556, step = 10900 (17.138 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 11000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.43199\n",
            "INFO:tensorflow:loss = 0.6558713, step = 11000 (22.562 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.89292\n",
            "INFO:tensorflow:loss = 0.7735438, step = 11100 (16.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.88539\n",
            "INFO:tensorflow:loss = 0.7232076, step = 11200 (16.992 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82256\n",
            "INFO:tensorflow:loss = 0.8281718, step = 11300 (17.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.91103\n",
            "INFO:tensorflow:loss = 0.74390906, step = 11400 (16.918 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87325\n",
            "INFO:tensorflow:loss = 0.6866874, step = 11500 (17.027 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86448\n",
            "INFO:tensorflow:loss = 0.69035405, step = 11600 (17.052 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.86891\n",
            "INFO:tensorflow:loss = 0.62324035, step = 11700 (17.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.85559\n",
            "INFO:tensorflow:loss = 0.7157076, step = 11800 (17.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90028\n",
            "INFO:tensorflow:loss = 0.6555241, step = 11900 (16.949 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 12000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "INFO:tensorflow:global_step/sec: 4.5227\n",
            "INFO:tensorflow:loss = 0.76905805, step = 12000 (22.111 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.87371\n",
            "INFO:tensorflow:loss = 0.5429293, step = 12100 (17.025 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.82346\n",
            "INFO:tensorflow:loss = 0.5931836, step = 12200 (17.171 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.90602\n",
            "INFO:tensorflow:loss = 0.6133048, step = 12300 (16.933 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.95075\n",
            "INFO:tensorflow:loss = 0.48602623, step = 12400 (16.804 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.81751\n",
            "INFO:tensorflow:loss = 0.46022436, step = 12500 (17.190 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8143\n",
            "INFO:tensorflow:loss = 0.36878344, step = 12600 (17.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.84875\n",
            "INFO:tensorflow:loss = 0.7375673, step = 12700 (17.097 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.96731\n",
            "INFO:tensorflow:loss = 0.62401426, step = 12800 (16.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 5.8535\n",
            "INFO:tensorflow:loss = 0.5867998, step = 12900 (17.084 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 13000 into /content/train_dir/model.ckpt.\n",
            "INFO:tensorflow:Reading data files from /content/data_dir/summarize_patent-dev*\n",
            "INFO:tensorflow:partition: 0 num_data_files: 10\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
            "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.label_smoothing to 0.0\n",
            "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
            "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
            "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
            "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_30619_512.bottom\n",
            "INFO:tensorflow:Transforming feature 'targets' with symbol_modality_30619_512.targets_bottom\n",
            "INFO:tensorflow:Building model body\n",
            "INFO:tensorflow:Transforming body output with symbol_modality_30619_512.top\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-06-07T22:09:05Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2019-06-07 22:09:05.930310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-07 22:09:05.930430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-07 22:09:05.930448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-07 22:09:05.930458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-07 22:09:05.930636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14325 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "INFO:tensorflow:Restoring parameters from /content/train_dir/model.ckpt-13000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [10/100]\n",
            "INFO:tensorflow:Evaluation [20/100]\n",
            "INFO:tensorflow:Evaluation [30/100]\n",
            "INFO:tensorflow:Evaluation [40/100]\n",
            "INFO:tensorflow:Evaluation [50/100]\n",
            "INFO:tensorflow:Evaluation [60/100]\n",
            "INFO:tensorflow:Evaluation [70/100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYhlzbWevNd6"
      },
      "source": [
        "# Data Generation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4twR84OXpKh"
      },
      "source": [
        "# Generate Data\n",
        "\"\"\"\n",
        "!t2t-datagen --t2t_usr_dir='/content' --data_dir='/content/data_dir' --tmp_dir='/content/tmp_dir' --problem=summarize_patent\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG_OrkSiLGZ-"
      },
      "source": [
        "# Training\n",
        "\n",
        "#--hparams_set=transformer_base_single_gpu\n",
        "#--hparams_set=transformer_tpu\n",
        "#--hparams_set=transformer_prepend\n",
        "\"\"\"\n",
        "!t2t-trainer --data_dir='/content/data_dir' --problem=summarize_patent \\\n",
        "    --model=transformer --hparams_set=transformer_base_single_gpu \\\n",
        "    --output_dir='/content/train_dir' \\\n",
        "    --hparams='batch_size=1024' \\\n",
        "    --t2t_usr_dir='/content' \\\n",
        "    --train_steps=140000\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8Qn9imCszOq"
      },
      "source": [
        "# Decoding (Inference)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJdTFn_Is8vU"
      },
      "source": [
        "# Take datapoints from an unseen dataset\n",
        "\"\"\"\n",
        "decode_file = u'/content/data_dir/sample_101010_dp_clean.csv'\n",
        "\n",
        "df = pd.read_csv(decode_file,sep='@',error_bad_lines=False, \n",
        "                     warn_bad_lines=False, header=0)   \n",
        "\n",
        "number_of_columns = len(list(df.columns))\n",
        "dp_test_data = int(df.size/number_of_columns)   #size = number of datapoints x (number of columns)\n",
        "start= dp_test_data - 2                         # ==> 1---1944--------------dp_test_data{3570} \n",
        "\n",
        "for i in range(start, dp_test_data): \n",
        "  dest_file = u'/content/data_dir/decode_data_{}.txt'.format(i)\n",
        "  abstract = str(df.loc[i,'Abstract'])   \n",
        "  claims = str(df.loc[i,'Claims'])\n",
        "  data = abstract + \" \" + claims  \n",
        "  with open(dest_file, 'w') as testdata:\n",
        "    testdata.write(data)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}